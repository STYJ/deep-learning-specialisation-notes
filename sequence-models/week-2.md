- Word embeddings allows your model to understand if words have any association.
- the 1 hot representation that we learnt last week is not able to understand relationship between words i.e. it is not able to generalise from learning a phrase that this other phrase might be also be worded this way.
- This is because you cannot calculate the distance between these any 2 words i.e. the inner product (distance) is 0.
- An alternative to this one hot representation is to use a featurized representation i.e. the words are the columns and the rows are the features e.g. gender, age, food, cost etc. This array has the shape of (# of features, # of words)
- You can turn this feature representation into a 2D space to easily visualise which words are more closely associated to each other. A common algorithm to do this is the t-SNE algorithm.
- You can use word embeddings to train your model to recognise relationships / associations and then do transfer learning to get it to solve other problems e.g. named entity recognition, text summarization.
- encoding and embedding can kind of be used interchangeably. They just refer to some kind of representation for the data.
- Word embeddings can also help with understanding analogies. This is achieved by taking the difference of the vectors between the words and trying to find a pair that is the closest match to this vector. e.g. man is to woman is as king is to ? Take man's embedding - woman's embedding. Find a (king, X) pair where the difference vector is closest to the man-woman's vector.
- These "vectors" only work when you compare them in the original dimensional space i.e. if you apply the t-SNE algorithm, this approach described above doesn't work anymore. 
- You can use something like a cosine similarity to determine how similar X is to man, women and king. Essentially what it does is it finds the angle between the 2 vectors. If they are exactly the same i.e. they overlap each other, that means the angle is 0 and so the cosine similarity gives you 1. If they are not related at all, maybe the angle might be 90 so the cosine similarity gives you 0. If however they are inversely related then the angle between the 2 vectors is 180 degrees so the cosine similarity gives you a value of -1.
- The problem with using a traditional softmax function with the skipgram (part of word2vec, there's another one called CBOW) algorithm is that it is computationally very expensive. You can opt for a hierarchical softmax instead.
- Generating training set = select a context word (c) and a word (w) that lies around c and give it the label 1. Then pick K random words and give those a label 0. So for every positive example, you will have K negative examples.
- There's another relatively simple algorithm called GloVe algorithm but uh... I don't exactly know what I'm learning lol.
- The naive approach to sentiment classification is to just look at the frequency of words and maybe like get the sum or average of these vectors which you passes through a softmax to output the probability of each outcome (1-5 stars). A better approach is to use a RNN instead of summing / averaging them as it takes the sequence of words into account e.g. "not good" is bad, not good!!
- A big problem with word embedding is that it can reflect the gender, ethnicity, age, sexual and other biases of the text used to train the model. For instance, it might associate female with nurse which is clearly stereotyping. That said, there are some words that are inherently associated with certain genders e.g. grandfather, grandmother.