- Before deciding on whether or not you should do something to reduce bias / variance, you should take a step back and check if it's actually worth it. For example, if your cat classifier keeps falsely classifying dog pics as cats, you should see how much % of a random sample of mislabelled dev set examples are dogs. If it's only like 5%, then you know that even if you can get your cat classifier to never misclassify dogs, it will give you a max improvement of only 5% of whatever your error was e.g. if your error is 10% then solving this problem (which might take months) only gives you an improvement of 0.05%. 
- Evaluate multiple ideas in parallel to save time!
- DL algorithms are quite robust to random errors (like a small mistake by the labeller) but it's not very good against systematic errors (a labeller consistently labelling white dogs as cats)
- If there are data that is incorrectly labelled for dev / test set, you can take a similar approach above and add it as one of the potential ideas to do to reduce bias. Make sure that your test and dev set still come from the same distribution (i.e. if you gonna fix the mislablled data on the dev set, do it for the test set too!)
- Andrew still does manual error analysis and checks the data to see why the classifier is making the mistakes it is making.
- Note that sometimes even though there's clearly an idea with the highest potential improvement if you can fix it but you need to also consider the cost and time it takes to solve it!
- Sometimes having different distribution for the training vs dev + test set is okay esp. if what is different is actually representative of the data that you are supposed to be predicting e.g. the actual images that users upload. Forcing them to be "from the same distribution" by shuffling them all together doesn't make sense because the target is still off i.e. 99% of the training data is not representative of future data so the model is trying to aim at a target that is at the wrong place!
- If same distribution but high error on dev error, this is because variance is high (overfitted training set). If different distribution and error is high on dev set then you cannot come to the same conclusion!
- Sometimes you can use a training-dev set. This is different from a dev set because the data in this set is data that is from the same distribution from the training set and is used to measure the variance of the model. 
- Human error -> training set error tells you avoidable bias
- Training set error -> training dev set error tells you about variance
- Training set error -> dev error tells you about data mismatch (assuming different distribution for training and dev sets)
- Dev set error -> test set error (you shouldnt really be doing this actually) but this tells you the degree of overfitting to dev set.
- There aren't really many ways to address data mismatch other than doing a manual analysis to see why your training data and dev/test data is different and how to go about collecting more of the correct type of data.
- when trying to artificially create data e.g. voice command + car noise, make sure that your car noise is not repeated too often otherwise there's a chance your model might overfit to this car noise. To the human ears, we cannot tell a difference but the machines can!
- Transfer learning is basically about taking an existing trained model, remove the last layer (which is tuned specifically to solve a certain problem) and insert a new last layer with a different set of parameters to solve another problem!
- Transfer learning makes sense if you have a lot of data from the problem you are transferring from and a lot less data from the problem you are transferring to.
- Multi task learning is training 1 NN to do many tasks. This is basically the equivalent of having multiple NNs each assigned to do 1 task. The output of multitask learning is an image with multiply classification (your last layer is not a 1x1 but rather a nx1 vectors of whether or not each row is true). Unlike softmax, softmax is a multi class classification but each image is still 1 label.
- End to end deep learning really just means getting A LOT of data and trying to predict the output you want without having to break it down into sub problems.
- Sometimes it's better to break a problem (is this person an employee) into multiple sub problems (where is the face of this person, is this person an employee?) if you have more data for each sub problem. Another example could be using an x ray image of a hand to predict the age of the person. There's a lot less data for image -> age as compared to image -> bones -> age.